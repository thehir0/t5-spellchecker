{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Context-sensitive Spelling Correction\n",
    "\n",
    "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
    "\n",
    "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
    "\n",
    "Useful links:\n",
    "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
    "- [Norvig's dataset](https://norvig.com/big.txt)\n",
    "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
    "\n",
    "Grading:\n",
    "- 60 points - Implement spelling correction\n",
    "- 20 points - Justify your decisions\n",
    "- 20 points - Evaluate on a test set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement context-sensitive spelling correction\n",
    "\n",
    "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
    "\n",
    "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
    "\n",
    "You may also want to implement:\n",
    "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
    "- some recent (or not very recent) paper on this topic,\n",
    "- solution which takes into account keyboard layout and associated misspellings,\n",
    "- efficiency improvement to make the solution faster,\n",
    "- any other idea of yours to improve the Norvig’s solution.\n",
    "\n",
    "IMPORTANT:  \n",
    "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
    "- Your implementation\n",
    "- Analysis of why the implemented approach is suggested\n",
    "- Improvements of the original approach that you have chosen to implement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution is train encoder-decoder transformer for seq2seq spelling correction task\n",
    "\n",
    "Norvig's solution is quite simple without the use of neural approaches, in contrast to this solution, an idea emerged on how much more effective modern neural approaches are than old probabilistic approaches. \n",
    "\n",
    "The solution is not based on any article, all implementations are completely unique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"ag_news\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 120000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 7600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Tiger Put to the Ryder Cup Challenge (AP) AP - Tiger Woods has been chasing Jack Nicklaus in golf record books since he was a kid. When it comes to the Ryder Cup, though, Tiger doesn't mean Jack.</td>\n",
       "      <td>Sports</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Johansson Upsets Roddick at U.S. Open NEW YORK - Andy Roddick ran into a bold, bigger version of himself at the U.S. Open, and 6-foot-6 Joachim Johansson sent the defending champion home...</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Internet Turns 35, Still Work in Progress (AP) AP - Thirty-five years after computer scientists at UCLA linked two bulky computers using a 15-foot gray cable, testing a new way for exchanging data over networks, what would ultimately become the Internet remains a work in progress.</td>\n",
       "      <td>Sci/Tech</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Disconnected PDAs are dead, according to RIM International wireless solutions manufacturer Research in Motion (RIM) believes the days of disconnected PDAs are gone. The BlackBerry-maker said that users #39; information is changing too rapidly for disconnected</td>\n",
       "      <td>Business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Doctors told to let baby Charlotte die The parents of a premature baby have lost their battle to force doctors to keep tiny 11-month-old Charlotte alive if she stops breathing a fourth time.</td>\n",
       "      <td>World</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_random_elements(dataset['train'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\")\n",
    "max_number_of_tokens = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "\n",
    "basicTokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def random_augmented(word, probability=0.4):\n",
    "    if np.random.choice([False, True], p=[1 - probability, probability]):\n",
    "        augmentation_type = np.random.choice(['delete', 'transpose', 'replace', 'insert'], p=[0.25, 0.25, 0.25, 0.25])\n",
    "        \n",
    "        if augmentation_type == 'delete':\n",
    "            \n",
    "            if len(word) < 2:\n",
    "                return word\n",
    "            \n",
    "            ind = np.random.randint(0, len(word) - 1)\n",
    "            return word[:ind] + word[ind + 1:]\n",
    "        \n",
    "        elif augmentation_type == 'transpose':\n",
    "            \n",
    "            if len(word) < 3:\n",
    "                return word\n",
    "            \n",
    "            ind = np.random.randint(1, len(word) - 1)\n",
    "            L, R = word[:ind], word[ind:]\n",
    "            return L + R[1] + R[0] + R[2:]\n",
    "        \n",
    "        elif augmentation_type == 'replace':\n",
    "            c = chr(ord('a') + np.random.randint(0, 26))\n",
    "            ind = np.random.randint(0, len(word))\n",
    "            return word[:ind] + c + word[ind + 1:]\n",
    "        else:\n",
    "            c = chr(ord('a') + np.random.randint(0, 26))\n",
    "            ind = np.random.randint(0, len(word) + 1)\n",
    "            return word[:ind] + c + word[ind:]\n",
    "\n",
    "    else:\n",
    "        return word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(batch):\n",
    "\n",
    "    batch_correct, batch_augmented = [], []\n",
    "\n",
    "    for sentence in batch:\n",
    "        tokenized_sentence = basicTokenizer.tokenize(sentence.lower())\n",
    "        augmented_sentence = list(map(random_augmented, tokenized_sentence))\n",
    "        batch_correct.append(' '.join(tokenized_sentence))\n",
    "        batch_augmented.append(' '.join(augmented_sentence))\n",
    "    \n",
    "    batch_correct = tokenizer(batch_correct, padding='max_length', truncation=True, max_length=max_number_of_tokens, return_tensors='pt',)\n",
    "    batch_augmented = tokenizer(batch_augmented, padding='max_length', truncation=True, max_length=max_number_of_tokens, return_tensors='pt')\n",
    "    \n",
    "    batch_correct['input_ids'][batch_correct['attention_mask'] == 0] = -100\n",
    "\n",
    "    batch_augmented['labels'] = batch_correct['input_ids']\n",
    "    return batch_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 20\n",
    "\n",
    "train_dataloader = DataLoader(dataset['train']['text'], shuffle=True, batch_size=batch_size, collate_fn=custom_collate)\n",
    "eval_dataloader = DataLoader(dataset['test']['text'], batch_size=batch_size, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([20, 64]), 'attention_mask': torch.Size([20, 64]), 'labels': torch.Size([20, 64])}\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    print({k: v.shape for k, v in batch.items()})\n",
    "    # print(model(**batch_correct).logits.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acc_score(preds, labels):\n",
    "\n",
    "    max_len = max(len(preds), len(labels))\n",
    "    preds.extend([''] * (max_len - len(preds)))\n",
    "    labels.extend([''] * (max_len - len(labels)))\n",
    "\n",
    "    acc = 0\n",
    "    for pred, label in zip(preds, labels):\n",
    "        acc += (pred == label)\n",
    "    \n",
    "    return acc / len(preds)\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    \n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    acc = []\n",
    "\n",
    "    for pred, label in zip(decoded_preds, decoded_labels):\n",
    "        pred = pred.split()\n",
    "        label = label.split()\n",
    "\n",
    "        acc.append(acc_score(pred, label))\n",
    "\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    ";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60000/60000 [1:47:02<00:00,  9.30it/s]"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_scheduler\n",
    "\n",
    "def train(train_dataloader, model, num_epochs = 10):\n",
    "    \n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "    lr_scheduler = get_scheduler(\n",
    "        name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "    )\n",
    "\n",
    "    progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch in train_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            progress_bar.update(1)\n",
    "    \n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, f'checkpoint-epoch-{epoch}.pt')\n",
    "train(train_dataloader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint = torch.load('checkpoint.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 380/380 [09:06<00:00,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.6805173401695055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def eval(model, eval_dataloader):\n",
    "    model.eval()\n",
    "\n",
    "    mean_acc = []\n",
    "\n",
    "    for batch in tqdm(eval_dataloader):\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**batch, max_new_tokens=max_number_of_tokens, num_beams=4, do_sample=True, temperature=1.1)\n",
    "\n",
    "        labels = batch['labels'].to('cpu')\n",
    "        mean_acc.extend(compute_metrics((outputs, labels)))\n",
    "    print(f'Accuracy:{sum(mean_acc) / len(mean_acc)}')\n",
    "eval(model, eval_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune on google_wellformed_query dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_query_dataset = load_dataset(\"google_wellformed_query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8750/8750 [14:56<00:00,  9.76it/s]\n"
     ]
    }
   ],
   "source": [
    "train(DataLoader(google_query_dataset['train']['content'], shuffle=True, batch_size=batch_size, collate_fn=custom_collate), model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 188/188 [02:43<00:00,  1.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:0.7454319263128661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval(model, DataLoader(google_query_dataset['validation']['content'], batch_size=batch_size, collate_fn=custom_collate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Justify your decisions\n",
    "\n",
    "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
    "- Which ngram dataset to use\n",
    "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
    "- Beam search parameters\n",
    "- etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The model was trained on two datasets:\n",
    "\n",
    "1. [Ag_news](https://huggingface.co/datasets/ag_news) \n",
    "\n",
    "    AG is a collection of more than 1 million news articles. News articles have been gathered from more than 2000 news sources.\n",
    "\n",
    "    ### Data Splits\n",
    "\n",
    "    | name  |train |test|\n",
    "    |-------|-----:|---:|\n",
    "    |default|120000|7600|\n",
    "    \n",
    "&nbsp;\n",
    "\n",
    "2. [Google wellformed query](https://huggingface.co/datasets/google_wellformed_query)\n",
    "\n",
    "    Google's query wellformedness dataset was created by crowdsourcing well-formedness annotations for 25,100 queries from the Paralex corpus\n",
    "\n",
    "    ### Data Splits\n",
    "\n",
    "    | name  |train |valid|\n",
    "    |-------|-----:|---:|\n",
    "    |default|17500|3750|\n",
    "\n",
    "### How to create spelling mistake?\n",
    "\n",
    "The approach is to generate some modificationsof word with an optional probability parameter (default: 0.4) which determines the likelihood of applying a modification to the word. The types of modifications(all edit distance 1) include deletion, transposition, replacement, and insertion. The probabilities for each type of modification are equal.\n",
    "\n",
    "Here's a brief overview of what each modification does:\n",
    "\n",
    "1. **Deletion**: Randomly deletes one character from the word, unless the word has only one character.\n",
    "2. **Transposition**: Randomly selects a character in the word (except the first and last characters) and swaps it with its adjacent character.\n",
    "3. **Replacement**: Randomly selects a character in the word and replaces it with a randomly chosen alphabet character.\n",
    "4. **Insertion**: Randomly inserts a randomly chosen alphabet character at a random position in the word.\n",
    "\n",
    "The reason to use only edit 1 is because edit distance 2 can cause huge deviations from the original word and edit distance 1 is the most common type of misspelling.\n",
    "\n",
    "### Further improvement\n",
    "\n",
    "1. Increase the edit distance depending on the length of the word.\n",
    "2. Train on more data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on a test set\n",
    "\n",
    "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norvig solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "WORDS = Counter(words(open('big.txt').read()))\n",
    "\n",
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test and compare two methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(example):\n",
    "    \n",
    "    lower = list(map(str.lower, example['content']))\n",
    "    tokenized = list(map(basicTokenizer.tokenize, lower))\n",
    "    augmented = [list(map(random_augmented, sentence)) for sentence in tokenized]\n",
    "    \n",
    "    mask = []\n",
    "    for sentence1, sentence2 in zip(tokenized, augmented):\n",
    "        mask.append([False if word1 == word2 else True for word1, word2 in zip(sentence1, sentence2)])\n",
    "\n",
    "    augmented = [' '.join(sentence) for sentence in tokenized]\n",
    "    tokenized = [' '.join(sentence) for sentence in tokenized]\n",
    "\n",
    "    example = example.add_column('augmented', augmented)\n",
    "    example = example.add_column('label', tokenized)\n",
    "    example = example.add_column('mask', mask)\n",
    "    \n",
    "    example = example.remove_columns(['content', 'rating'])\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_query_dataset['test'] = preprocess(google_query_dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['augmented', 'label', 'mask'],\n",
       "    num_rows: 3850\n",
       "})"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "google_query_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_collate(batch):\n",
    "\n",
    "    batch = {\n",
    "        'label': [dct['label'] for dct in batch],\n",
    "        'augmented': [dct['augmented'] for dct in batch],\n",
    "        'mask': [dct['mask'] for dct in batch],\n",
    "    }\n",
    "    \n",
    "    batch_augmented = tokenizer(batch['augmented'], padding='max_length', truncation=True, max_length=max_number_of_tokens, return_tensors='pt')\n",
    "    batch_augmented['labels'] = batch['label']\n",
    "    batch_augmented['mask'] = batch['mask']\n",
    "    return batch_augmented"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLoader = DataLoader(google_query_dataset['test'], batch_size=1, collate_fn=test_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_recall_score(preds, labels, mask):\n",
    "    \n",
    "    max_len = max(len(preds), len(labels))\n",
    "    preds.extend([''] * (max_len - len(preds)))\n",
    "    labels.extend([''] * (max_len - len(labels)))\n",
    "    mask.extend([False] * (max_len - len(mask)))\n",
    "    \n",
    "    # print(f'{preds=}')\n",
    "    # print(f'{labels=}')\n",
    "    # print(f'{mask=}')\n",
    "    \n",
    "    TP, FP, FN, TN = 0, 0, 0, 0\n",
    "\n",
    "    for pred, label, mask in zip(preds, labels, mask):\n",
    "        if pred == label:\n",
    "            if mask:\n",
    "                TP += 1\n",
    "            else:\n",
    "                TN += 1\n",
    "        else:\n",
    "            if mask:\n",
    "                FP += 1\n",
    "            else:\n",
    "                #mask False means input = label\n",
    "                FN += 1\n",
    "    return TP, FP, FN, TN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3850/3850 [10:44<00:00,  5.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer solution\n",
      "Recall:0.8646024665442141\n",
      "      Precision:0.9522204026587034\n",
      "Norvig \n",
      "Recall:0.8440992095938948\n",
      "      Precision:0.8950004816491668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "transformer_score, norvig_score = {'TP': 0, 'FP': 0, 'FN': 0, 'TN': 0}, {'TP': 0, 'FP': 0, 'FN': 0, 'TN': 0}\n",
    "\n",
    "for batch in tqdm(testLoader):\n",
    "    \n",
    "    label = basicTokenizer.tokenize(batch['labels'][0])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        input_ids = batch['input_ids'].to('cuda')\n",
    "        attention_mask = batch['attention_mask'].to('cuda')\n",
    "        outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=max_number_of_tokens, num_beams=4, do_sample=True, temperature=1.1)\n",
    "        \n",
    "        decoded = basicTokenizer.tokenize(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n",
    "        TP, FP, FN, TN = precision_recall_score(decoded, label, batch['mask'][0])\n",
    "        \n",
    "        transformer_score['TP'] += TP\n",
    "        transformer_score['FP'] += FP\n",
    "        transformer_score['FN'] += FN\n",
    "        transformer_score['TN'] += TN\n",
    "\n",
    "        tokenized = basicTokenizer.tokenize(tokenizer.batch_decode(batch['input_ids'], skip_special_tokens=True)[0])\n",
    "        norvig_output = list(map(correction, tokenized))\n",
    "\n",
    "        TP, FP, FN, TN = precision_recall_score(norvig_output, label, batch['mask'][0])\n",
    "        norvig_score['TP'] += TP\n",
    "        norvig_score['FP'] += FP\n",
    "        norvig_score['FN'] += FN\n",
    "        norvig_score['TN'] += TN\n",
    "\n",
    "\n",
    "print(f\"Transformer solution\\nRecall:{transformer_score['TP'] / (transformer_score['TP'] + transformer_score['FN'])}\\n\\\n",
    "      Precision:{transformer_score['TP'] / (transformer_score['TP'] + transformer_score['FP'])}\")\n",
    "print(f\"Norvig \\nRecall:{norvig_score['TP'] / (norvig_score['TP'] + norvig_score['FN'])}\\n\\\n",
    "      Precision:{norvig_score['TP'] / (norvig_score['TP'] + norvig_score['FP'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "\n",
    "You can also try your example using [HuggingFace Inference API](https://huggingface.co/the-hir0/google-t5-small-spellchecker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this is a japanese dull']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "promt = 'Thsi is a Japanees dull'\n",
    "\n",
    "input = tokenizer(promt, return_tensors='pt').to('cuda')\n",
    "\n",
    "output = model.generate(**input, max_new_tokens=max_number_of_tokens, num_beams=4, do_sample=True, temperature=1.1)\n",
    "tokenizer.batch_decode(output, skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
